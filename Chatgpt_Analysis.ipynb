{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 1.756169789624281\n",
      "p-value: 0.17297117872082665\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis Test Regarding Score and Thumb-up\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "df = pd.read_csv(\"Hypothesis_Test_Score_And_Thumb-up.csv\")\n",
    "\n",
    "group_1 = df[df['THUMBS_GROUP'] == '1-5']['SCORE']\n",
    "group_2 = df[df['THUMBS_GROUP'] == '6-20']['SCORE']\n",
    "group_3 = df[df['THUMBS_GROUP'] == '20+']['SCORE']\n",
    "\n",
    "# One-Way ANOVA\n",
    "f_stat, p_val = f_oneway(group_1, group_2, group_3)\n",
    "print(\"F-statistic:\", f_stat)\n",
    "print(\"p-value:\", p_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of reviews with non-ASCII characters: 21.68%\n",
      "Potential emoji-only or non-informative reviews: 2900\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"chatgpt_reviews_partial_100000.csv\")\n",
    "\n",
    "df = df[df['content'].notna()]\n",
    "df['content'] = df['content'].astype(str).str.strip()\n",
    "df = df[df['content'].str.len() > 0]\n",
    "def has_non_ascii(text):\n",
    "    return any(ord(char) > 127 for char in text)\n",
    "\n",
    "df['has_non_ascii'] = df['content'].apply(has_non_ascii)\n",
    "\n",
    "non_ascii_ratio = df['has_non_ascii'].mean()\n",
    "print(f\"Percentage of reviews with non-ASCII characters: {non_ascii_ratio:.2%}\")\n",
    "\n",
    "df['content_length'] = df['content'].str.len()\n",
    "emoji_like = df[(df['has_non_ascii']) & (df['content_length'] < 5)]\n",
    "print(f\"Potential emoji-only or non-informative reviews: {len(emoji_like)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Team Centric Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading snowflake_connector_python-3.17.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.8/73.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python[pandas])\n",
      "  Downloading boto3-1.40.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (1.34.69)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (42.0.5)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (24.0.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (2.32.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (4.11.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (3.13.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (3.10.0)\n",
      "Requirement already satisfied: tomlkit in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (0.11.1)\n",
      "Collecting pandas<3.0.0,>=2.1.2 (from snowflake-connector-python[pandas])\n",
      "  Using cached pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python[pandas]) (14.0.2)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python[pandas])\n",
      "  Downloading botocore-1.40.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python[pandas]) (1.0.1)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python[pandas])\n",
      "  Using cached s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python[pandas]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/anaconda3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python[pandas]) (2.2.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.21)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python[pandas]) (1.16.0)\n",
      "Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading boto3-1.40.11-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.40.11-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.3.1-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Downloading snowflake_connector_python-3.17.1-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: asn1crypto, pandas, botocore, s3transfer, boto3, snowflake-connector-python\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.69\n",
      "    Uninstalling botocore-1.34.69:\n",
      "      Successfully uninstalled botocore-1.34.69\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.40.11 which is incompatible.\n",
      "panel 1.4.4 requires bokeh<3.5.0,>=3.4.0, but you have bokeh 3.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asn1crypto-1.5.1 boto3-1.40.11 botocore-1.40.11 pandas-2.3.1 s3transfer-0.13.1 snowflake-connector-python-3.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"snowflake-connector-python[pandas]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.17.1\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "print(snowflake.connector.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: snowflake-connector-python in /opt/anaconda3/lib/python3.12/site-packages (3.17.1)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (14.0.2)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.11)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.40.11)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (42.0.5)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (24.0.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.32.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (4.11.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.13.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (3.10.0)\n",
      "Requirement already satisfied: tomlkit in /opt/anaconda3/lib/python3.12/site-packages (from snowflake-connector-python) (0.11.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/anaconda3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.2.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n",
      "Downloading scikit_learn-1.7.1-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-21.0.0-cp312-cp312-macosx_12_0_arm64.whl (31.2 MB)\n",
      "Installing collected packages: pyarrow, scikit-learn\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed pyarrow-21.0.0 scikit-learn-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn snowflake-connector-python pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/f6xlkj4n0v18gt_r7s7t1j700000gn/T/ipykernel_38355/1413911622.py:343: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ctx = pd.read_sql(\"SELECT CURRENT_USER() u, CURRENT_ROLE() r, CURRENT_DATABASE() db, CURRENT_SCHEMA() sch\", conn)\n",
      "/var/folders/0n/f6xlkj4n0v18gt_r7s7t1j700000gn/T/ipykernel_38355/1413911622.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          U            R                 DB     SCH\n",
      "MATTCAO2001 ACCOUNTADMIN CHATGPT_REVIEWS_DB STAGING\n",
      "Loaded 5,883 low-score rows (before filters)\n",
      "[Info] Selected K=12 (score=0.925)\n",
      "Saved: topics_keywords_20250819_185531.csv, topics_examples_20250819_185531.csv, topics_summary_20250819_185531.csv\n",
      "Wrote results to CHATGPT_REVIEWS_DB.STAGING.TOPICS_KEYWORDS / TOPICS_EXAMPLES / TOPICS_SUMMARY\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# analyze_topics\n",
    "\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from snowflake.connector import connect\n",
    "from sklearn.feature_extraction import text as sk_text\n",
    "\n",
    "# Settings & Parameters\n",
    "DB_NAME       = \"CHATGPT_REVIEWS_DB\"\n",
    "SCHEMA_NAME   = \"STAGING\"\n",
    "VIEW_NAME     = \"V_LOW_SCORE_CLEAN\"\n",
    "\n",
    "TABLE_TOPICS   = \"TOPICS_KEYWORDS\"\n",
    "TABLE_EXAMPLES = \"TOPICS_EXAMPLES\"\n",
    "TABLE_SUMMARY  = \"TOPICS_SUMMARY\"\n",
    "\n",
    "K_RANGE          = range(6, 13)      # Choose from 6 to 12\n",
    "TOPN_WORDS       = 12                \n",
    "EXAMPLES_PER_TP  = 5\n",
    "MIN_DF           = 12            # A word needs to be presented in at least 12 reviews to be kept \n",
    "MAX_DF           = 0.40\n",
    "MAX_FEATURES     = 50000\n",
    "EN_PROP_MIN      = 0.60\n",
    "\n",
    "MMR_LAMBDA       = 0.7           # The larger the value, the more emphasis on relevance; the smaller the value, the more emphasis on diversity.\n",
    "DEDUP_JACCARD    = 0.90          # Delete duplicate (when similarity is greater than or equal to 0.9, we see it as duplicated reviews) \n",
    "\n",
    "\n",
    "# Adjust when needed\n",
    "DOMAIN_STOP = {\n",
    "    \"app\",\"apps\",\"chatgpt\",\"openai\",\"ai\",\"gpt\",\"version\",\"versions\",\"update\",\"updated\",\n",
    "    \"fix\",\"fixed\",\"issue\",\"issues\",\"problem\",\"problems\",\"bug\",\"bugs\",\"please\",\"pls\",\n",
    "    \"thanks\",\"thank\",\"hi\",\"hello\",\"team\",\"dear\",\"experience\",\"experiences\",\n",
    "    \"nice\",\"good\",\"bad\",\"very\",\"best\",\"worst\",\"useful\",\"useless\",\"helpful\",\"not\",\n",
    "    \"really\",\"actually\",\"also\",\"ever\",\"always\",\"never\",\"still\",\"just\",\"well\",\"ok\",\n",
    "    \"work\",\"works\",\"working\",\"worked\",\"proper\",\"properly\",\"application\"\n",
    "}\n",
    "STOP_WORDS = list(sk_text.ENGLISH_STOP_WORDS.union(DOMAIN_STOP))\n",
    "\n",
    "CANON_MAP = {\n",
    "    r\"\\blog in\\b\": \"login\", r\"\\blogin\\b\": \"login\", r\"\\bsign in\\b\": \"login\",\n",
    "    r\"\\b2fa\\b\": \"mfa\", r\"\\b2-factor\\b\": \"mfa\", r\"\\b2 factor\\b\": \"mfa\",\n",
    "    r\"\\bslow\\b\": \"lag\", r\"\\blaggy\\b\": \"lag\", r\"\\blag\\b\": \"lag\",\n",
    "    r\"\\bprice\\b\": \"pricing\", r\"\\bcharged?\\b\": \"billing\",\n",
    "    r\"\\brefunds?\\b\": \"refund\", r\"\\bsubscription\\b\": \"subscribe\",\n",
    "    r\"\\bcrash(es|ed|ing)?\\b\": \"crash\",\n",
    "    r\"\\bdoesn['’]?t work\\b\": \"not_working\",\n",
    "    r\"\\bisn['’]?t working\\b\": \"not_working\",\n",
    "    r\"\\bnot working\\b\": \"not_working\",\n",
    "}\n",
    "\n",
    "NAME_RULES = [\n",
    "    (\"Login / Account\",     [\"login\",\"account\",\"password\",\"otp\",\"mfa\",\"verification\"]),\n",
    "    (\"Billing / Subscription\",[\"billing\",\"subscribe\",\"payment\",\"pricing\",\"refund\",\"charge\"]),\n",
    "    (\"Performance / Crash\", [\"lag\",\"slow\",\"freeze\",\"loading\",\"crash\",\"latency\"]),\n",
    "    (\"Answer Quality\",      [\"answer\",\"response\",\"quality\",\"accuracy\",\"wrong\",\"hallucination\"]),\n",
    "    (\"Access / Region\",     [\"region\",\"country\",\"available\",\"access\",\"blocked\",\"unsupported\"]),\n",
    "    (\"UI / Usability\",      [\"ui\",\"ux\",\"button\",\"menu\",\"dark mode\",\"layout\"]),\n",
    "]\n",
    "\n",
    "# Snowflake \n",
    "def load_env():\n",
    "    load_dotenv(override=True)\n",
    "    for k in [\"SNOWFLAKE_USER\",\"SNOWFLAKE_PASSWORD\",\"SNOWFLAKE_ACCOUNT\"]:\n",
    "        if not os.getenv(k): raise RuntimeError(f\"Missing .env key: {k}\")\n",
    "    return {\n",
    "        \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "        \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"role\": os.getenv(\"SNOWFLAKE_ROLE\", \"SYSADMIN\"),\n",
    "        \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\"),\n",
    "        \"database\": os.getenv(\"SNOWFLAKE_DATABASE\", DB_NAME),\n",
    "        \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\", SCHEMA_NAME),\n",
    "    }\n",
    "\n",
    "def connect_sf(cfg):\n",
    "    conn = connect(user=cfg[\"user\"], password=cfg[\"password\"], account=cfg[\"account\"],\n",
    "                   warehouse=cfg[\"warehouse\"], database=cfg[\"database\"],\n",
    "                   schema=cfg[\"schema\"], role=cfg[\"role\"])\n",
    "    with conn.cursor() as cs:\n",
    "        cs.execute(f\"USE ROLE {cfg['role']}\")\n",
    "        cs.execute(f\"USE WAREHOUSE {cfg['warehouse']}\")\n",
    "        cs.execute(f\"USE DATABASE {DB_NAME}\")\n",
    "        cs.execute(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "    return conn\n",
    "\n",
    "def fetch_df(conn):\n",
    "    sql = f\"\"\"\n",
    "      SELECT review_id, content, thumbs_up_count, app_version, review_time\n",
    "      FROM {DB_NAME}.{SCHEMA_NAME}.{VIEW_NAME}\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# Data Cleaning \n",
    "# Data cleaning is before TF-IDF Vectorization\n",
    "def english_prop(s: str) -> float:\n",
    "    s = str(s or \"\")\n",
    "    letters = sum(ch.isalpha() for ch in s)\n",
    "    en_letters = sum('a' <= ch.lower() <= 'z' for ch in s)\n",
    "    return (en_letters / letters) if letters else 0.0\n",
    "\n",
    "def canon_replace(text: str) -> str:\n",
    "    t = text\n",
    "    for pat, rep in CANON_MAP.items(): t = re.sub(pat, rep, t)\n",
    "    return t\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    t = str(s or \"\").lower()\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \" \", t)         # Get rid of URL\n",
    "    t = canon_replace(t)                          # Synonym normalization\n",
    "    t = re.sub(r\"[^a-z0-9\\s']\", \" \", t)          # English letters / digits / spaces / apostrophes only\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# Similarity & MMR (select representative reviews)\n",
    "def _tokens_for_jaccard(text: str):\n",
    "    return {w for w in re.findall(r\"[a-zA-Z]{2,}\", (text or \"\").lower()) if w not in STOP_WORDS}\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a or not b: return 0.0\n",
    "    return len(a & b) / len(a | b)\n",
    "\n",
    "def mmr_select(texts, rel_scores, top_k, lam=MMR_LAMBDA):\n",
    "    chosen, cand = [], list(range(len(texts)))\n",
    "    token_sets = [_tokens_for_jaccard(t) for t in texts]\n",
    "    if not cand: return []\n",
    "    first = int(np.argmax(rel_scores)); chosen.append(first); cand.remove(first)\n",
    "    while len(chosen) < min(top_k, len(texts)) and cand:\n",
    "        best_i, best_score = None, -1e9\n",
    "        for i in cand:\n",
    "            sim_to_chosen = 0.0 if not chosen else max(jaccard(token_sets[i], token_sets[j]) for j in chosen)\n",
    "            score = lam * rel_scores[i] - (1 - lam) * sim_to_chosen\n",
    "            if score > best_score: best_score, best_i = score, i\n",
    "        chosen.append(best_i); cand.remove(best_i)\n",
    "    return chosen\n",
    "\n",
    "# Topics Naming \n",
    "def _is_english_phrase(s: str) -> bool:\n",
    "    # Allow only A–Z and spaces (compatible with bigram/trigram)\n",
    "    return bool(re.fullmatch(r\"[A-Za-z]+(?: [A-Za-z]+)*\", s or \"\"))\n",
    "\n",
    "def auto_name(keywords: list[str]) -> str:\n",
    "    kw_join = \" \".join(keywords)\n",
    "    for name, needles in NAME_RULES:\n",
    "        if any(n in kw_join for n in needles): return name\n",
    "    # Fallback: use English phrases only\n",
    "    eng = [w for w in keywords if _is_english_phrase(w)]\n",
    "    return \", \".join(eng[:3]).title() if eng else \"General\"\n",
    "\n",
    "# Choose K(6 to 12) \n",
    "def score_topics(H):\n",
    "    sim = cosine_similarity(H)            # we want this to be small (similarity between different topics should be small)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    inter_sim = sim.mean()\n",
    "    sparsity = (H < (H.mean(axis=1, keepdims=True))).mean()  # we want this to be large (the fewer words that can represent a topic, the better)\n",
    "    return (1 - inter_sim) * 0.6 + sparsity * 0.4        # score which serves as proof to determine which K should we choose\n",
    "\n",
    "def fit_nmf_with_best_k(X):\n",
    "    best = None\n",
    "    for k in K_RANGE:\n",
    "        nmf = NMF(n_components=k, init=\"nndsvda\", random_state=42, max_iter=400)\n",
    "        W = nmf.fit_transform(X); H = nmf.components_          # X = W*H\n",
    "        s = score_topics(H)\n",
    "        if (best is None) or (s > best[\"score\"]): best = {\"k\": k, \"model\": nmf, \"W\": W, \"H\": H, \"score\": s}\n",
    "    return best\n",
    "\n",
    "# Main pipeline\n",
    "def topic_pipeline(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    for col in [\"content\",\"review_id\",\"thumbs_up_count\",\"app_version\",\"review_time\"]:\n",
    "        if col not in df.columns: raise KeyError(f\"missing column: {col}\")\n",
    "\n",
    "    df[\"en_prop\"] = df[\"content\"].apply(english_prop)\n",
    "    df = df[df[\"en_prop\"] >= EN_PROP_MIN]\n",
    "    df[\"text_clean\"] = df[\"content\"].apply(clean_text)\n",
    "    df = df[df[\"text_clean\"].str.len() >= 10]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if df.empty: return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Vectorize only English words (≥2 letters)\n",
    "    vec = TfidfVectorizer(\n",
    "        ngram_range=(1,3),\n",
    "        min_df=MIN_DF, max_df=MAX_DF, max_features=MAX_FEATURES,\n",
    "        stop_words=STOP_WORDS, sublinear_tf=True,\n",
    "        token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b'\n",
    "    )\n",
    "    X = vec.fit_transform(df[\"text_clean\"])\n",
    "    terms = vec.get_feature_names_out()\n",
    "    if X.shape[0] < min(K_RANGE): raise ValueError(f\"Too few documents ({X.shape[0]}) for topic modeling.\")\n",
    "\n",
    "    best = fit_nmf_with_best_k(X)\n",
    "    nmf, W, H, k = best[\"model\"], best[\"W\"], best[\"H\"], best[\"k\"]\n",
    "    print(f\"[Info] Selected K={k} (score={best['score']:.3f})\")\n",
    "\n",
    "    # Topic keywords (English phrases only)\n",
    "    topics_rows, name_map = [], {}\n",
    "    for t_idx in range(k):\n",
    "        idx = np.argsort(H[t_idx])[::-1][:TOPN_WORDS]\n",
    "        cand = [terms[i] for i in idx]\n",
    "        kws_eng = [w for w in cand if _is_english_phrase(w) and w.lower() not in STOP_WORDS][:TOPN_WORDS]\n",
    "        topics_rows.append({\"topic\": int(t_idx), \"keywords\": \", \".join(kws_eng)})\n",
    "        name_map[t_idx] = auto_name(kws_eng)\n",
    "    topics_df = pd.DataFrame(topics_rows)\n",
    "\n",
    "    # Representative Reviews\n",
    "    df[\"topic\"] = W.argmax(axis=1)\n",
    "    rows = []\n",
    "    for t_idx in range(k):\n",
    "        sub = df[df[\"topic\"] == t_idx].copy()\n",
    "        if sub.empty: continue\n",
    "        pos = df.index.get_indexer(sub.index)\n",
    "        weights = W[pos, t_idx]\n",
    "        rep = weights * np.log1p(sub[\"thumbs_up_count\"].fillna(0))\n",
    "        sub = sub.assign(rep=rep)\n",
    "\n",
    "        # Remove Duplicates\n",
    "        uniq, seen = [], []\n",
    "        for _, r in sub.sort_values(\"rep\", ascending=False).iterrows():\n",
    "            toks = _tokens_for_jaccard(r[\"content\"])\n",
    "            if any(jaccard(toks, s) >= DEDUP_JACCARD for s in seen): continue\n",
    "            seen.append(toks); uniq.append(r)\n",
    "        if not uniq: continue\n",
    "\n",
    "        texts = [str(r[\"content\"]) for r in uniq]\n",
    "        rels  = [float(r[\"rep\"]) for r in uniq]\n",
    "        picks = mmr_select(texts, rels, EXAMPLES_PER_TP, lam=MMR_LAMBDA)\n",
    "        for j in picks:\n",
    "            r = uniq[j]\n",
    "            rows.append({\n",
    "                \"topic\": int(t_idx),\n",
    "                \"topic_name\": name_map[t_idx],\n",
    "                \"review_id\": r[\"review_id\"],\n",
    "                \"thumbs_up_count\": int(r.get(\"thumbs_up_count\") or 0),\n",
    "                \"app_version\": r.get(\"app_version\"),\n",
    "                \"review_time\": r.get(\"review_time\"),\n",
    "                \"example_review\": r[\"content\"],\n",
    "                \"rep_score\": float(r[\"rep\"])\n",
    "            })\n",
    "    examples_df = pd.DataFrame(rows)\n",
    "\n",
    "    # PM Summary\n",
    "    total = len(df)\n",
    "    summary_rows = []\n",
    "    for t_idx in range(k):\n",
    "        sub = df[df[\"topic\"] == t_idx]\n",
    "        if len(sub) == 0: continue\n",
    "        share = len(sub) / total\n",
    "        thumbs = sub[\"thumbs_up_count\"].fillna(0)\n",
    "        summary_rows.append({\n",
    "            \"topic\": int(t_idx),\n",
    "            \"topic_name\": name_map[t_idx],\n",
    "            \"share_pct\": round(share * 100, 2),         # percentage of a given topic\n",
    "            \"thumbs_avg\": float(thumbs.mean()),         # thumb-up average\n",
    "            \"thumbs_median\": float(thumbs.median()),    # thumb-up median\n",
    "            \"sample_size\": int(len(sub))\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"topic\")  \n",
    "\n",
    "    return topics_df, examples_df, summary_df\n",
    "\n",
    "# Table: create table + auto add missing columns\n",
    "def ensure_tables(conn):\n",
    "    with conn.cursor() as cs:\n",
    "        cs.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {DB_NAME}.{SCHEMA_NAME}.{TABLE_TOPICS} (\n",
    "          run_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "          topic NUMBER,\n",
    "          keywords STRING\n",
    "        )\"\"\")\n",
    "        cs.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {DB_NAME}.{SCHEMA_NAME}.{TABLE_EXAMPLES} (\n",
    "          run_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "          topic NUMBER,\n",
    "          review_id STRING,\n",
    "          thumbs_up_count NUMBER,\n",
    "          app_version STRING,\n",
    "          review_time TIMESTAMP_NTZ,\n",
    "          example_review STRING\n",
    "        )\"\"\")\n",
    "        cs.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY} (\n",
    "          run_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "          topic NUMBER\n",
    "        )\"\"\")\n",
    "        # auto add missing columns\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_EXAMPLES} ADD COLUMN IF NOT EXISTS TOPIC_NAME  STRING\")\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_EXAMPLES} ADD COLUMN IF NOT EXISTS REP_SCORE   FLOAT\")\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY}  ADD COLUMN IF NOT EXISTS TOPIC_NAME   STRING\")\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY}  ADD COLUMN IF NOT EXISTS SHARE_PCT    FLOAT\")\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY}  ADD COLUMN IF NOT EXISTS THUMBS_AVG   FLOAT\")\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY}  ADD COLUMN IF NOT EXISTS THUMBS_MEDIAN FLOAT\")\n",
    "        cs.execute(f\"ALTER TABLE {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY}  ADD COLUMN IF NOT EXISTS SAMPLE_SIZE  NUMBER\")\n",
    "\n",
    "def to_iso_ntz(x):\n",
    "    if pd.isna(x): return None\n",
    "    ts = pd.to_datetime(x, errors=\"coerce\")\n",
    "    if pd.isna(ts): return None\n",
    "    try: ts = ts.tz_convert(None)\n",
    "    except Exception: pass\n",
    "    return ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def write_back(conn, topics_df, examples_df, summary_df):\n",
    "    if not examples_df.empty and \"review_time\" in examples_df.columns:\n",
    "        examples_df = examples_df.copy()\n",
    "        examples_df[\"review_time\"] = examples_df[\"review_time\"].apply(to_iso_ntz)\n",
    "\n",
    "    with conn.cursor() as cs:\n",
    "        for tbl in [TABLE_TOPICS, TABLE_EXAMPLES, TABLE_SUMMARY]:\n",
    "            cs.execute(f\"DELETE FROM {DB_NAME}.{SCHEMA_NAME}.{tbl} WHERE DATE(run_at)=CURRENT_DATE()\")\n",
    "\n",
    "        if not topics_df.empty:\n",
    "            cs.executemany(\n",
    "                f\"INSERT INTO {DB_NAME}.{SCHEMA_NAME}.{TABLE_TOPICS}(topic, keywords) VALUES (%(topic)s, %(keywords)s)\",\n",
    "                topics_df.to_dict(\"records\"))\n",
    "\n",
    "        if not examples_df.empty:\n",
    "            cs.executemany(\n",
    "                f\"\"\"INSERT INTO {DB_NAME}.{SCHEMA_NAME}.{TABLE_EXAMPLES}\n",
    "                (topic, topic_name, review_id, thumbs_up_count, app_version, review_time, example_review, rep_score)\n",
    "                VALUES (%(topic)s, %(topic_name)s, %(review_id)s, %(thumbs_up_count)s, %(app_version)s, %(review_time)s, %(example_review)s, %(rep_score)s)\"\"\",\n",
    "                examples_df.to_dict(\"records\"))\n",
    "\n",
    "        if not summary_df.empty:\n",
    "            cs.executemany(\n",
    "                f\"\"\"INSERT INTO {DB_NAME}.{SCHEMA_NAME}.{TABLE_SUMMARY}\n",
    "                (topic, topic_name, share_pct, thumbs_avg, thumbs_median, sample_size)\n",
    "                VALUES (%(topic)s, %(topic_name)s, %(share_pct)s, %(thumbs_avg)s, %(thumbs_median)s, %(sample_size)s)\"\"\",\n",
    "                summary_df.to_dict(\"records\"))\n",
    "\n",
    "# Main Process\n",
    "def main():\n",
    "    cfg  = load_env()\n",
    "    conn = connect_sf(cfg)\n",
    "\n",
    "    ctx = pd.read_sql(\"SELECT CURRENT_USER() u, CURRENT_ROLE() r, CURRENT_DATABASE() db, CURRENT_SCHEMA() sch\", conn)\n",
    "    print(ctx.to_string(index=False))\n",
    "\n",
    "    df = fetch_df(conn)\n",
    "    print(f\"Loaded {len(df):,} low-score rows (before filters)\")\n",
    "\n",
    "    topics_df, examples_df, summary_df = topic_pipeline(df)\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    p1, p2, p3 = (f\"topics_keywords_{ts}.csv\",\n",
    "                  f\"topics_examples_{ts}.csv\",\n",
    "                  f\"topics_summary_{ts}.csv\")\n",
    "    topics_df.to_csv(p1, index=False)\n",
    "    examples_df.to_csv(p2, index=False)\n",
    "    summary_df.to_csv(p3, index=False)\n",
    "    print(f\"Saved: {p1}, {p2}, {p3}\")\n",
    "\n",
    "    ensure_tables(conn)\n",
    "    write_back(conn, topics_df, examples_df, summary_df)\n",
    "    print(f\"Wrote results to {DB_NAME}.{SCHEMA_NAME}.{TABLE_TOPICS} / {TABLE_EXAMPLES} / {TABLE_SUMMARY}\")\n",
    "\n",
    "    conn.close(); print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
